{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4703bfd-eaff-46cf-a4b9-f3d85cd824aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy pandas matplotlib seaborn tensorflow pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c5a8e-c998-460b-85a3-602513bbcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ecf55-b1cc-4254-842a-b99747866769",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index))\n",
    "\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebfa3d8-9182-4277-9d4b-b3180ff6d9cd",
   "metadata": {},
   "source": [
    "## Represent data\n",
    "\n",
    "The following code cell creates preprocessing layers outputting three features:\n",
    "\n",
    "* `latitude` X `longitude` (a feature cross)\n",
    "* `median_income`\n",
    "* `population`\n",
    "\n",
    "This code cell specifies the features that you'll ultimately train the model on and how each of those features will be represented. The transformations (collected in `prepocessing_layers`) don't actually get applied until you pass a DataFrame to it, which will happen when we train the model.\n",
    "\n",
    "We'll use `preprocessing_layers` for both our linear regression model and our neural network model.\n",
    "\n",
    "(The [`keras.FeatureSpace`](https://keras.io/examples/structured_data/structured_data_classification_with_feature_space) utility offers an alternative to building individual Keras preprocessing layers -- give it a try, if you're feeling adventurous!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2c344-36a9-4c1c-8f6f-5aac24743825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Input tensors of float values.\n",
    "inputs = {\n",
    "    'latitude':\n",
    "        tf.keras.layers.Input(\n",
    "            shape=(1,),\n",
    "            dtype=tf.float32,\n",
    "            name='latitude'),\n",
    "    'longitude':\n",
    "        tf.keras.layers.Input(\n",
    "            shape=(1,),\n",
    "            dtype=tf.float32,\n",
    "            name='longitude'),\n",
    "    'median_income':\n",
    "        tf.keras.layers.Input(\n",
    "            shape=(1,),\n",
    "            dtype=tf.float32,\n",
    "            name='median_income'),\n",
    "    'population':\n",
    "        tf.keras.layers.Input(\n",
    "            shape=(1,),\n",
    "            dtype=tf.float32,\n",
    "            name='population')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97502d08-b2bd-4572-b97b-cb544c587157",
   "metadata": {},
   "source": [
    "# Normalization Layers\n",
    "\n",
    "This layer will shift and scale inputs into a distribution centered around 0 with standard deviation 1. It accomplishes this by precomputing the mean and variance of the data, and calling `(input - mean) / sqrt(var)` at runtime.\n",
    "\n",
    "The mean and variance values for the layer must be either supplied on construction or learned via `adapt()`. `adapt()` will compute the mean and variance of the data and store them as the layer's weights. `adapt()` should be called before `fit()`, `evaluate()`, or `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7a0c1-4540-4c72-8a91-9433bb19e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Normalization layer to normalize the median_income data.\n",
    "median_income = tf.keras.layers.Normalization(\n",
    "    name='normalization_median_income',\n",
    "    axis=None)\n",
    "median_income.adapt(train_df['median_income'])\n",
    "median_income = median_income(inputs.get('median_income'))\n",
    "\n",
    "# Create a Normalization layer to normalize the population data.\n",
    "population = tf.keras.layers.Normalization(\n",
    "    name='normalization_population',\n",
    "    axis=None)\n",
    "population.adapt(train_df['population'])\n",
    "population = population(inputs.get('population'))\n",
    "\n",
    "# Create a Normalization layer to normalize the latitude data.\n",
    "latitude = tf.keras.layers.Normalization(\n",
    "    name='normalization_latitude',\n",
    "    axis=None)\n",
    "latitude.adapt(train_df['latitude'])\n",
    "latitude = latitude(inputs.get('latitude'))\n",
    "\n",
    "# Create a Normalization layer to normalize the longitude data.\n",
    "longitude = tf.keras.layers.Normalization(\n",
    "    name='normalization_longitude',\n",
    "    axis=None)\n",
    "longitude.adapt(train_df['longitude'])\n",
    "longitude = longitude(inputs.get('longitude'))\n",
    "\n",
    "# Create Normalization layers to normalize the median_house_value data.\n",
    "# Because median_house_value is our label (i.e., the target value we're\n",
    "# predicting), these layers won't be added to our model.\n",
    "train_median_house_value_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "train_median_house_value_normalized.adapt(\n",
    "    np.array(train_df['median_house_value']))\n",
    "\n",
    "test_median_house_value_normalized = tf.keras.layers.Normalization(axis=None)\n",
    "test_median_house_value_normalized.adapt(\n",
    "    np.array(test_df['median_house_value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280ba45-f02b-4448-b8a3-bb0fc17dcf72",
   "metadata": {},
   "source": [
    "# Bucketizing Values\n",
    "\n",
    "Create a list of numbers representing the bucket boundaries for latitude. Because we're using a Normalization layer, values for latitude and longitude\n",
    "will be in the range of approximately -3 to 3 (representing the Z score). We'll create 20 buckets, which requires 21 bucket boundaries (hence, 20+1).\n",
    "\n",
    "To do this, we'll use a `Discretization()` layer, which will place each element of its input data into one of several contiguous ranges and output an integer index indicating which range each element was placed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660c2dc-edba-4d20-b4fe-76eeecdecc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_boundaries = np.linspace(-3, 3, 20+1)\n",
    "longitude_boundaries = np.linspace(-3, 3, 20+1)\n",
    "\n",
    "# Create a Discretization layer to separate the latitude data into buckets.\n",
    "latitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=latitude_boundaries,\n",
    "    name='discretization_latitude')(latitude)\n",
    "\n",
    "# Create a Discretization layer to separate the longitude data into buckets.\n",
    "longitude = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=longitude_boundaries,\n",
    "    name='discretization_longitude')(longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7cc7b-725a-4b5b-b9d4-520919c3823b",
   "metadata": {},
   "source": [
    "# Feature Crosses\n",
    "\n",
    "This layer performs crosses of categorical features using the \"hashing trick\". Conceptually, the transformation can be thought of as: `hash(concatenate(features)) % num_bins`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc70d6-3ebb-4530-9938-8988d3f9ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross the latitude and longitude features into a single one-hot vector.\n",
    "feature_cross = tf.keras.layers.HashedCrossing(\n",
    "    # num_bins can be adjusted: Higher values improve accuracy, lower values\n",
    "    # improve performance.\n",
    "    num_bins=len(latitude_boundaries) * len(longitude_boundaries),\n",
    "    output_mode='one_hot',\n",
    "    name='cross_latitude_longitude')([latitude, longitude])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ec7ff-c597-4d30-9e27-e45c04b7d3d1",
   "metadata": {},
   "source": [
    "# Concatenation\n",
    "\n",
    "Finally, we will combine all three of our inputs into a single tensor using a `Concatenate()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d1603-a449-4b9e-9ff6-cf0a8b10221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate our inputs into a single tensor.\n",
    "preprocessing_layers = tf.keras.layers.Concatenate()(\n",
    "    [feature_cross, median_income, population])\n",
    "\n",
    "dense_output = tf.keras.layers.Dense(\n",
    "    units=1,\n",
    "    name='dense_output')(preprocessing_layers)\n",
    "\n",
    "# Define an output dictionary we'll send to the model constructor.\n",
    "outputs = {\n",
    "'dense_output': dense_output\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab141791-0962-4e8d-9f1d-ae3c44cb181e",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Basline Linear Regression\n",
    "\n",
    "Let's first train a multiple linear regression on the preprocessing layers we just created. This will give us a good baseline model to compare our neural network against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dddb633-cba2-428f-ae0d-2d401fe6cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 1000\n",
    "label_name = \"median_house_value\"\n",
    "\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053eb14-6789-428e-b1f4-19895b82f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "lin_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "lin_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e7f35-20cf-43b2-8c62-53aed3ab9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and label.\n",
    "features = {name:np.array(value) for name, value in train_df.items()}\n",
    "label = train_median_house_value_normalized(\n",
    "    np.array(features.pop(label_name))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523ebf8-5348-4d23-8a03-2bbe99c54645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = lin_model.fit(\n",
    "    x=features,\n",
    "    y=label,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    validation_split=validation_split)\n",
    "\n",
    "# Get details that will be useful for plotting the loss curve.\n",
    "epochs = history.epoch\n",
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist[\"mean_squared_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ecd1a-81cf-4658-b212-ef7ab661921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "plt.plot(epochs, mse, label=\"Training Loss\")\n",
    "plt.plot(epochs, history.history[\"val_mean_squared_error\"], label=\"Validation Loss\")\n",
    "\n",
    "# mse_training is a pandas Series, so convert it to a list first.\n",
    "merged_mse_lists = mse.tolist() + history.history[\"val_mean_squared_error\"]\n",
    "highest_loss = max(merged_mse_lists)\n",
    "lowest_loss = min(merged_mse_lists)\n",
    "top_of_y_axis = highest_loss * 1.03\n",
    "bottom_of_y_axis = lowest_loss * 0.97\n",
    "\n",
    "plt.ylim([bottom_of_y_axis, top_of_y_axis])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d3b15-b6a3-458b-8f91-1d17344ed1e7",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Train a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf9b52-3bc2-4042-9188-0987f00d54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output = tf.keras.layers.Dense(\n",
    "    units=20,\n",
    "    activation='relu',\n",
    "    name='hidden_dense_layer_1')(preprocessing_layers)\n",
    "\n",
    "# Create a Dense layer with 12 nodes.\n",
    "dense_output = tf.keras.layers.Dense(\n",
    "    units=12,\n",
    "    activation='relu',\n",
    "    name='hidden_dense_layer_2')(dense_output)\n",
    "\n",
    "# Create the Dense output layer.\n",
    "dense_output = tf.keras.layers.Dense(\n",
    "    units=1,\n",
    "    name='dense_output')(dense_output)\n",
    "\n",
    "# Define an output dictionary we'll send to the model constructor.\n",
    "outputs = {\n",
    "'dense_output': dense_output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58288a-23b7-46e5-a162-d2330f9488aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "dnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9893fc-fe1e-4b85-abf9-ca7b60be71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(dnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94911b02-bbdc-4d85-b931-182dac6628b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 1000\n",
    "label_name = \"median_house_value\"\n",
    "\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf58316-811b-40e5-a195-c2a346dcfc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dnn_model.fit(\n",
    "    x=features,\n",
    "    y=label,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    validation_split=validation_split)\n",
    "\n",
    "# Get details that will be useful for plotting the loss curve.\n",
    "epochs = history.epoch\n",
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist[\"mean_squared_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c142bed-94b9-4a21-b9d0-f8fae89cd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "plt.plot(epochs, mse, label=\"Training Loss\")\n",
    "plt.plot(epochs, history.history[\"val_mean_squared_error\"], label=\"Validation Loss\")\n",
    "\n",
    "# mse_training is a pandas Series, so convert it to a list first.\n",
    "merged_mse_lists = mse.tolist() + history.history[\"val_mean_squared_error\"]\n",
    "highest_loss = max(merged_mse_lists)\n",
    "lowest_loss = min(merged_mse_lists)\n",
    "top_of_y_axis = highest_loss * 1.03\n",
    "bottom_of_y_axis = lowest_loss * 0.97\n",
    "\n",
    "plt.ylim([bottom_of_y_axis, top_of_y_axis])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60525f66-9826-4c08-a721-f1a4f79dafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = test_median_house_value_normalized(np.array(test_features.pop(label_name)))\n",
    "\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "dnn_model.evaluate(\n",
    "    x=test_features,\n",
    "    y=test_label,\n",
    "    batch_size=batch_size,\n",
    "    return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e61163-480f-4f5f-a910-e4a170df43a9",
   "metadata": {},
   "source": [
    "# Lab Exercise\n",
    "\n",
    "Craigslist is the world's largest collection of used vehicles for sale. GitHub user [@AustinReese](https://github.com/AustinReese/UsedVehicleSearch) scraped Craigslist used car ads to create a dataset of 4.25 million listings. We will be using a subset of around 2 million of these listings.\n",
    "\n",
    "For this lab:\n",
    "\n",
    "  1. Clean and preprocess your data:\n",
    "      - Create a training dataset that includes `price`, `ageOfCar` (in years), `manufacturer` (dummy-encoded), `condition` (dummy-encoded), `odometer`, and `isCarvana` (0 or 1)\n",
    "      - Be sure to remove any outliers from your dataset as you go\n",
    "  2. Define a feed-forward neural network\n",
    "      - It should take `ageOfCar`, `manufacturer`, `condition`, `odometer`, and `isCarvana` as inputs\n",
    "      - It should have 2 hidden layers\n",
    "          - How many neurons do you think each layer should include? Why?\n",
    "          - What activation function(s) do you want to use in each layer?\n",
    "      - It should use MSE as its loss function\n",
    "      - The final output layer should be a single neuron, and should use either a ReLU or linear activation function\n",
    "  3. Train your model and evaluate its performance between the training and validation samples\n",
    "  4. Evaluate your model against the holdout testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e7836-cafe-4556-8bb5-54d36929b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://cdn.c18l.org/vehicles_lab.csv\")\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57e0c44-b5b9-4ffd-8a36-ee3ff2f99867",
   "metadata": {},
   "source": [
    "# Transformers for generative text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3b9c4-878b-40de-ba53-a50ca04c460f",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to create and train a [sequence-to-sequence](https://developers.google.com/machine-learning/glossary#sequence-to-sequence-task) [Transformer](https://developers.google.com/machine-learning/glossary#Transformer) model to create a chatbot based on comment threads taken from the popular subreddit, r/science. The Transformer was originally proposed in [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017).\n",
    "\n",
    "Transformers are deep neural networks that replace CNNs and RNNs with [self-attention](https://developers.google.com/machine-learning/glossary#self-attention). Self attention allows Transformers to easily transmit information across the input sequences.\n",
    "\n",
    "This tutorial draws heavily from Google's [Transformers tutorial for text translation](https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54e9d0-7d04-4532-882c-b00cfe145595",
   "metadata": {},
   "source": [
    "That's a lot to digest, the goal of this tutorial is to break it down into easy to understand parts. In this tutorial you will:\n",
    "\n",
    "- Prepare the data.\n",
    "- Implement necessary components:\n",
    "  - Positional embeddings.\n",
    "  - Attention layers.\n",
    "  - The encoder and decoder.\n",
    "- Build & train the Transformer.\n",
    "- Interact with your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a0ec3-a2e2-4617-8bfb-e530c6be2f77",
   "metadata": {},
   "source": [
    "## Why Transformers are significant\n",
    "\n",
    "- Transformers excel at modeling sequential data, such as natural language.\n",
    "- Unlike the [recurrent neural networks (RNNs)](./text_generation.ipynb), Transformers are parallelizable. This makes them efficient on hardware like GPUs and TPUs. The main reasons is that Transformers replaced recurrence with attention, and computations can happen simultaneously. Layer outputs can be computed in parallel, instead of a series like an RNN.\n",
    "- Unlike [RNNs](https://www.tensorflow.org/guide/keras/rnn) (like [seq2seq, 2014](https://arxiv.org/abs/1409.3215)) or [convolutional neural networks (CNNs)](https://www.tensorflow.org/tutorials/images/cnn) (for example, [ByteNet](https://arxiv.org/abs/1610.10099)), Transformers are able to capture distant or long-range contexts and dependencies in the data between distant positions in the input or output sequences. Thus, longer connections can be learned. Attention allows each location to have access to the entire input at each layer, while in RNNs and CNNs, the information needs to pass through many processing steps to move a long distance, which makes it harder to learn.\n",
    "- Transformers make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, [StarCraft units](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii)).\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/encoder_self_attention_distribution.png\" width=\"800\" alt=\"Encoder self-attention distribution for the word it from the 5th to the 6th layer of a Transformer trained on English-to-French translation\">\n",
    "\n",
    "Figure: The encoder self-attention distribution for the word \"it\" from the 5th to the 6th layer of a Transformer trained on English-to-French translation (one of eight attention heads). Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd282f-4a0e-4a3e-aba9-44d5d063db59",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ae51d-34bc-47e6-bb02-71bc3560bf91",
   "metadata": {},
   "source": [
    "We'll begin by installing the required packages. For this tutorial we will use the Tensorflow Text package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a0353-dd3f-478d-81b3-cc2a258b38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-text tensorflow-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6172a4-2c7f-49dc-93a9-3304d098ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8852bc-a6fa-482d-a5e5-a8c56ae87744",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd474b-a60a-4c2a-8edc-6799a021e407",
   "metadata": {},
   "source": [
    "For this lab, we will use a dataset of 1,500,000 comments taken from various posts on the popular subreddit, r/science. These comments are part of the larger [Reddit corpus created by Cornell](https://convokit.cornell.edu/documentation/subreddit.html).\n",
    "\n",
    "I have already done some preprocessing of these data, so we can just load them in as a TensorFlow Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aab984-a95c-43b4-a27c-ead45d8bc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.load('/home/chris/projects/data-2000/labs/data/records.tfdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e5f2f-4abe-41c3-881f-03cc2c38a076",
   "metadata": {},
   "source": [
    "The `tf.data.Dataset` object returned by TensorFlow Datasets yields pairs of text examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309048ef-42e2-42f2-8f2c-348a192a0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "for questions, answers in data.batch(3).take(1):\n",
    "    print('> Example questions:')\n",
    "    for q in questions.numpy():\n",
    "        print(q.decode('utf-8'))\n",
    "    print()\n",
    "    \n",
    "    print('> Example answers:')\n",
    "    for a in answers.numpy():\n",
    "        print(a.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f841c9-d8bc-4818-b627-ac81da21805b",
   "metadata": {},
   "source": [
    "## Set up the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f57bdd-58d0-47ae-af6f-914cbc54b248",
   "metadata": {},
   "source": [
    "Now that you have loaded the dataset, you need to tokenize the text, so that each element is represented as a [token](https://developers.google.com/machine-learning/glossary#token) or token ID (a numeric representation).\n",
    "\n",
    "This tutorial uses the tokenizers built in the [subword tokenizer](https://www.tensorflow.org/text/guide/subwords_tokenizer) tutorial. That tutorial optimizes a `text.BertTokenizer` object for a machine translation dataset and exports it in a TensorFlow `saved_model` format. It's important to note that we are reusing this pre-trained tokenizer for simplicity, and that for more accurate results we could instead train a new tokenizer that is specific to our dataset.\n",
    "\n",
    "> Note: This is different from the [original paper](https://arxiv.org/pdf/1706.03762.pdf), section 5.1, where they used a single byte-pair tokenizer for both the source and target with a vocabulary-size of 37000.\n",
    "\n",
    "Download, extract, and import the `saved_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828ded3-b99f-4d3d-be8a-55015a28c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "tf.keras.utils.get_file(\n",
    "    f'{model_name}.zip',\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50d06e-1c5e-4f79-9a23-97d74b521af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.saved_model.load(model_name)\n",
    "tokenizer = tokenizers.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc97a4-8f2d-40af-9169-2185a583f8da",
   "metadata": {},
   "source": [
    "The `tokenize` method converts a batch of strings to a padded-batch of token IDs. This method splits punctuation, lowercases and unicode-normalizes the input before tokenizing. That standardization is not visible here because the input data is already standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fcdadb-a259-47b5-8823-ce7d1ff93306",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.tokenize(questions)\n",
    "\n",
    "print('> This is a padded-batch of token IDs:')\n",
    "for row in encoded.to_list():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd6f70-8316-41b8-8b5d-cfb09093586d",
   "metadata": {},
   "source": [
    "The `detokenize` method attempts to convert these token IDs back to human-readable text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec70ba2d-1cf6-4fee-b9aa-a914aac3d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_trip = tokenizer.detokenize(encoded)\n",
    "\n",
    "print('> This is human-readable text:')\n",
    "for line in round_trip.numpy():\n",
    "    print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9467b08-b0f5-4a5f-9121-865b4be07e4d",
   "metadata": {},
   "source": [
    "The distribution of tokens per example in the dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c84f2f-b216-40a8-9b79-402ab7c4f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "for questions, answers in data.batch(5000):\n",
    "    tokens = tokenizer.tokenize(questions + answers)\n",
    "    lengths.append(tokens.row_lengths())\n",
    "    print('.', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fb65c-f526-4ae9-ac86-9a5b11389344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "all_lengths = np.concatenate(lengths)\n",
    "\n",
    "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
    "plt.ylim(plt.ylim())\n",
    "max_length = max(all_lengths)\n",
    "plt.plot([max_length, max_length], plt.ylim())\n",
    "plt.title(f'Maximum tokens per example: {max_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6032f92-0aaa-4bc3-9b6f-7bb5e720091b",
   "metadata": {},
   "source": [
    "### Set up a data pipeline with `tf.data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76ab7d-e62a-4c5f-8c42-a63fb18e6f42",
   "metadata": {},
   "source": [
    "The following function takes batches of text as input, and converts them to a format suitable for training. \n",
    "\n",
    "1. It tokenizes them into ragged batches.\n",
    "2. It trims each to be no longer than `MAX_TOKENS`.\n",
    "3. It splits the target (English) tokens into inputs and labels. These are shifted by one step so that at each input location the `label` is the id of the next token.\n",
    "4. It converts the `RaggedTensor`s to padded dense `Tensor`s.\n",
    "5. It returns an `(inputs, labels)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd900ae8-43b2-4ea6-80c0-75b888bca581",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=256\n",
    "def prepare_batch(q, a):\n",
    "    q = tokenizer.tokenize(q)      # Output is ragged.\n",
    "    q = q[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    q = q.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    a = tokenizer.tokenize(a)\n",
    "    a = a[:, :(MAX_TOKENS+1)]\n",
    "    a_inputs = a[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    a_labels = a[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return (q, a_inputs), a_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027adb91-62c3-4360-88ca-12af7e113c4c",
   "metadata": {},
   "source": [
    "The function below converts a dataset of text examples into data of batches for training. \n",
    "\n",
    "1. It tokenizes the text, and filters out the sequences that are too long.\n",
    "   (The `batch`/`unbatch` is included because the tokenizer is much more efficient on large batches).\n",
    "2. The `cache` method ensures that that work is only executed once.\n",
    "3. Then `shuffle` and, `dense_to_ragged_batch` randomize the order and assemble batches of examples. \n",
    "4. Finally `prefetch` runs the dataset in parallel with the model to ensure that data is available when needed. See [Better performance with the `tf.data`](https://www.tensorflow.org/guide/data_performance.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0986ad-2b1f-4661-b980-c52161a696f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaccb91-4674-4972-aaaf-6d797f0c1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9473af-caeb-4f97-8120-8108a93374a4",
   "metadata": {},
   "source": [
    "## Test the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e039bc-9bab-4ecf-915e-ae4e68b89ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = make_batches(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99911d3c-98ef-42ac-a839-513f6bff49c8",
   "metadata": {},
   "source": [
    "The resulting `tf.data.Dataset` objects are setup for training with Keras.\n",
    "Keras `Model.fit` training expects `(inputs, labels)` pairs.\n",
    "The `inputs` are pairs of tokenized comment and response (or question and answer) sequences, `(q, a)`.\n",
    "The `labels` are the same response sequences shifted by 1.\n",
    "This shift is so that at each location input `q` sequence, the `label` is the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac8312-b442-43ab-803a-b0bb3f142b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (q, a), a_labels in train_batches.take(1):\n",
    "  break\n",
    "\n",
    "print(q.shape)\n",
    "print(a.shape)\n",
    "print(a_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931c0fc-1c9e-4a87-a494-6acc1d11eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0][:10])\n",
    "print(a_labels[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faffa1dc-4432-413e-999e-c048c46690aa",
   "metadata": {},
   "source": [
    "# Define the model components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ab6ed-4894-419f-9a07-de9b85f1ea8e",
   "metadata": {},
   "source": [
    "There's a lot going on inside a Transformer. The important things to remember are:\n",
    "\n",
    "1. It follows the same general pattern as a standard sequence-to-sequence model with an encoder and a decoder.\n",
    "2. If you work through it step by step it will all make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df5ebb-c75a-4595-948e-5e9ecd05a4f7",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The original Transformer diagram</th>\n",
    "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Each of the components in these two diagrams will be explained as we work through the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9286f0-6928-4a60-b765-268dfb43ab97",
   "metadata": {},
   "source": [
    "## The Embedding and Positional Encoding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b39cac-3685-45e3-8883-90410cce7479",
   "metadata": {},
   "source": [
    "The inputs to both the encoder and decoder use the same embedding and positional encoding logic. \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The embedding and positional encoding layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefedd4-3682-447b-b977-dc5699227b0b",
   "metadata": {},
   "source": [
    "Given a sequence of tokens, both the input tokens (comments) and target tokens (responses) have to be converted to vectors using a `tf.keras.layers.Embedding` layer.\n",
    "\n",
    "The attention layers used throughout the model see their input as a set of vectors, with no order. Since the model doesn't contain any recurrent or convolutional layers. It needs some way to identify word order, otherwise it would see the input sequence as a [bag of words](https://developers.google.com/machine-learning/glossary#bag-of-words) instance, `how are you`, `how you are`, `you how are`, and so on, are indistinguishable.\n",
    "\n",
    "A Transformer adds a \"Positional Encoding\" to the embedding vectors. It uses a set of sines and cosines at different frequencies (across the sequence). By definition nearby elements will have similar position encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420fec8-63b6-4bb8-85b4-239f06661fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    \n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    \n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "    \n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1) \n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b973fa-304d-43a5-bc61-3ba078e4f4d9",
   "metadata": {},
   "source": [
    "We can use this to create a `PositionEmbedding` layer that looks-up a token's embedding vector and adds the position vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b69c7-f588-42ce-a6db-29962a88dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            d_model,\n",
    "            mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(\n",
    "            length=2048,\n",
    "            depth=d_model)\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the\n",
    "        # embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69172c55-2bea-4d0c-a2b2-c97d0c8549ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_q = PositionalEmbedding(vocab_size=tokenizer.get_vocab_size(), d_model=512)\n",
    "embed_a = PositionalEmbedding(vocab_size=tokenizer.get_vocab_size(), d_model=512)\n",
    "\n",
    "q_emb = embed_q(q)\n",
    "a_emb = embed_a(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f4982-7c77-4536-a208-27ed54ad1357",
   "metadata": {},
   "source": [
    "### Add and normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29618205-72b7-483d-afbd-6e952c600bea",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th colspan=2>Add and normalize</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "These \"Add & Norm\" blocks are scattered throughout the model. Each one joins a residual connection and runs the result through a `LayerNormalization` layer.\n",
    "\n",
    "The easiest way to organize the code is around these residual blocks. The following sections will define custom layer classes for each. \n",
    "\n",
    "The residual \"Add & Norm\" blocks are included so that training is efficient. The residual connection provides a direct path for the gradient (and ensures that vectors are **updated** by the attention layers instead of **replaced**), while the normalization maintains a reasonable scale for the outputs.\n",
    "\n",
    "Note: The implementations, below, use the `Add` layer to ensure that Keras masks are propagated (the `+` operator does not)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc77bf-9026-4242-a201-c8c5a36bcd05",
   "metadata": {},
   "source": [
    "## The Base Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ba10c-d67c-479a-a63d-c2652cd6298b",
   "metadata": {},
   "source": [
    "Attention layers are used throughout the model. These are all identical except for how the attention is configured. Each one contains a `layers.MultiHeadAttention`, a `layers.LayerNormalization` and a `layers.Add`. \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=2>The base attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "To implement these attention layers, start with a simple base class that just contains the component layers. Each use-case will be implemented as a subclass. It's a little more code to write this way, but it keeps the intention clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94753eb8-c26b-4a94-a491-7dedf80feb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820927c-aea9-4f0c-b8fa-970b14518a43",
   "metadata": {},
   "source": [
    "### How attention works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f669fa-116b-4c3f-b121-48d94ceaa96e",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The base attention layer</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "There are two inputs:\n",
    "\n",
    "1. The query sequence; the sequence being processed; the sequence doing the attending (bottom).\n",
    "2. The context sequence; the sequence being attended to (left).\n",
    "\n",
    "The output has the same shape as the query-sequence.\n",
    "\n",
    "The common comparison is that this operation is like a dictionary lookup.\n",
    "A **fuzzy**, **differentiable**, **vectorized** dictionary lookup.\n",
    "\n",
    "Here's a regular python dictionary, with 3 keys and 3 values being passed a single query.\n",
    "\n",
    "```\n",
    "d = {'color': 'blue', 'age': 22, 'type': 'pickup'}\n",
    "result = d['color']\n",
    "```\n",
    "\n",
    "- The `query` is what you're trying to find.\n",
    "- The `key` is what sort of information the dictionary has.\n",
    "- The `value` is that information.\n",
    "\n",
    "When you look up a `query` in a regular dictionary, the dictionary finds the matching `key`, and returns its associated `value`.\n",
    "The `query` either has a matching `key` or it doesn't.\n",
    "You can imagine a **fuzzy** dictionary where the keys don't have to match perfectly.\n",
    "If you looked up `d[\"species\"]` in the dictionary above, maybe you'd want it to return `\"pickup\"` since that's the best match for the query.\n",
    "\n",
    "An attention layer does a fuzzy lookup like this, but it's not just looking for the best key.\n",
    "It combines the `values` based on how well the `query` matches each `key`.\n",
    "\n",
    "How does that work? In an attention layer the `query`, `key`, and `value` are each vectors.\n",
    "Instead of doing a hash lookup the attention layer combines the `query` and `key` vectors to determine how well they match, the \"attention score\".\n",
    "The layer returns the average across all the `values`, weighted by the \"attention scores\".\n",
    "\n",
    "Each location the query-sequence provides a `query` vector.\n",
    "The context sequence acts as the dictionary. At each location in the context sequence provides a `key` and `value` vector.\n",
    "The input vectors are not used directly, the `layers.MultiHeadAttention` layer includes `layers.Dense` layers to project the input vectors before using them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bab152-fba9-447c-8e73-457a251fab4e",
   "metadata": {},
   "source": [
    "## The Cross-Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315fe9c-3c15-4aba-9de5-57d2beec2279",
   "metadata": {},
   "source": [
    "At the literal center of the Transformer is the cross-attention layer. This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block in the [NMT with attention tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention).\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The cross attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646c013-2815-41aa-8b3f-2a177a9f7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "       \n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "    \n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4bfc4-cd81-46cc-baed-cb922c2f7e9b",
   "metadata": {},
   "source": [
    "## The Global Self-Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd397f4-68fe-4597-800d-2f53c657753b",
   "metadata": {},
   "source": [
    "This layer is responsible for processing the context sequence, and propagating information along its length:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The global self attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Since the context sequence is fixed while the translation is being generated, information is allowed to flow in both directions. \n",
    "\n",
    "The global self attention layer lets every sequence element directly access every other sequence element, with only a few operations, and all the outputs can be computed in parallel. \n",
    "\n",
    "To implement this layer you just need to pass the target sequence, `x`, as both the `query`, and `value` arguments to the `mha` layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c870d28-bd06-4988-b0b9-7cc84368d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3098971-bf7f-4262-9af2-510d4b64b1de",
   "metadata": {},
   "source": [
    "## The Causal Self Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026ccb7-875f-46f4-897c-5bf54983f31d",
   "metadata": {},
   "source": [
    "This layer does a similar job as the global self attention layer, for the output sequence:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The causal self attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "This needs to be handled differently from the encoder's global self attention layer.  \n",
    "\n",
    "Like the [text generation tutorial](https://www.tensorflow.org/text/tutorials/text_generation), and the [NMT with attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention) tutorial, Transformers are an \"autoregressive\" model: They generate the text one token at a time and feed that output back to the input. To make this _efficient_, these models ensure that the output for each sequence element only depends on the previous sequence elements; the models are \"causal\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e745155-8f21-43d2-a74b-4655d5b37b31",
   "metadata": {},
   "source": [
    "A causal model is efficient in two ways: \n",
    "\n",
    "1. In training, it lets you compute loss for every location in the output sequence while executing the model just once.\n",
    "2. During inference, for each new token generated you only need to calculate its outputs, the outputs for the previous sequence elements can be reused.\n",
    "  - For an RNN you just need the RNN-state to account for previous computations (pass `return_state=True` to the RNN layer's constructor).\n",
    "  - For a CNN you would need to follow the approach of [Fast Wavenet](https://arxiv.org/abs/1611.09482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f141405-68a8-42d4-b685-d68fb6064555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6794dff1-f67d-4b6c-aa9a-0fa5a3887516",
   "metadata": {},
   "source": [
    "The causal mask ensures that each location only has access to the locations that come before it: \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The causal self attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726d0d9-4586-40bb-a73f-0cc5f757b5fe",
   "metadata": {},
   "source": [
    "## The Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306981e-da49-4440-beea-75e92076cdaa",
   "metadata": {},
   "source": [
    "The transformer also includes this point-wise feed-forward network in both the encoder and decoder:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The feed forward network</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The network consists of two linear layers (`tf.keras.layers.Dense`) with a ReLU activation in-between, and a dropout layer. As with the attention layers the code here also includes the residual connection and normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98be620-1bca-4c96-9812-b699be4c5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2868729-2cc3-4cb7-975a-ffd70bdacd60",
   "metadata": {},
   "source": [
    "# Assembling the Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14312ce-6a72-4113-af20-5e121ef81c83",
   "metadata": {},
   "source": [
    "## The Encoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b1d06-ba62-4b6a-a2e7-6ec807d87c9c",
   "metadata": {},
   "source": [
    "The encoder contains a stack of `N` encoder layers. Where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The encoder layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b239d15-5ec0-40a3-92c9-dc52429c363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "    \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd8279-22a1-4b7a-aafc-0fbeb845e698",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383c4d3-3265-412c-9471-982b5656fd14",
   "metadata": {},
   "source": [
    "The encoder consists of:\n",
    "\n",
    "- A `PositionalEmbedding` layer at the input.\n",
    "- A stack of `EncoderLayer` layers.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The encoder</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129484b-535d-4bd5-b404-276639e7cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model)\n",
    "    \n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(\n",
    "                d_model=d_model,\n",
    "                num_heads=num_heads,\n",
    "                dff=dff,\n",
    "                dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        # Shape `(batch_size, seq_len, d_model)`.\n",
    "        x = self.pos_embedding(x)\n",
    "        \n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "    \n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c111b-5ec8-4321-a3fe-2d9224f1ffc8",
   "metadata": {},
   "source": [
    "## The Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd11f7d-535d-4891-8674-cb8f9e8250af",
   "metadata": {},
   "source": [
    "The decoder's stack is slightly more complex, with each `DecoderLayer` containing a `CausalSelfAttention`, a `CrossAttention`, and a `FeedForward` layer:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The decoder layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c13f1-16c9-480b-9f47-d110f253a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dff,\n",
    "        dropout_rate=0.1\n",
    "    ):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "    \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "    \n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "    \n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eeebdd-c649-4f9f-8bb5-18f7d055218b",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4c20f-4393-423b-916e-298fa7c779fe",
   "metadata": {},
   "source": [
    "Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`s:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The embedding and positional encoding layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Decoder.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169725ee-6bb2-4b40-85ee-936f71ad9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, *, num_layers, d_model, num_heads, dff,\n",
    "        vocab_size, dropout_rate=0.1\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(\n",
    "                d_model=d_model,\n",
    "                num_heads=num_heads,\n",
    "                dff=dff,\n",
    "                dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "    \n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "    \n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "    \n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d65399-956b-4ade-8429-1e241cdf705b",
   "metadata": {},
   "source": [
    "# The Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e24f3-c152-4a82-b019-381448bc7d6e",
   "metadata": {},
   "source": [
    "<img src=\"https://media3.giphy.com/media/fl9WeVqXsGmHK/giphy.gif?cid=ecf05e47chz0jj60h32ebeu0134fv77pe84vfbfjpsmhgb5y&ep=v1_gifs_search&rid=giphy.gif&ct=g\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f14e2-90c4-420b-8dc9-5a5cd6dbce8b",
   "metadata": {},
   "source": [
    "You now have `Encoder` and `Decoder`. To complete the `Transformer` model, you need to put them together and add a final linear (`Dense`) layer which converts the resulting vector at each location into output token probabilities. \n",
    "\n",
    "The output of the decoder is the input to this final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d1494-c75c-42b2-bff6-0b6b90e7bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "                   input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "    \n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "    \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        context, x  = inputs\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "        \n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "        \n",
    "        # Final linear layer output.\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "        \n",
    "        try:\n",
    "        # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "        # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        # Return the final output and the attention weights.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f808e24-4a46-468a-9a3a-88977181039d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7badfb4-7cca-45ae-8f65-dd3cab2e8379",
   "metadata": {},
   "source": [
    "To keep this example small and relatively fast, the number of layers (`num_layers`), the dimensionality of the embeddings (`d_model`), and the internal dimensionality of the `FeedForward` layer (`dff`) have been reduced.\n",
    "\n",
    "The base model described in the original Transformer paper used `num_layers=6`, `d_model=512`, and `dff=2048`.\n",
    "\n",
    "The number of self-attention heads remains the same (`num_heads=8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdb2d2-dee9-484e-9134-0b696c411dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe5f83-575b-4f65-870b-58376d3f6753",
   "metadata": {},
   "source": [
    "## Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3617e0-4736-46f3-b101-dd7863ab66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizer.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizer.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2c75a-14dc-4920-b91b-720644ff0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer((q, a))\n",
    "\n",
    "print(q.shape)\n",
    "print(a.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe0d6a-b7f7-4a39-b98f-9068fff3c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
    "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23516814-686f-44c6-9190-8a1387da137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725aeeb9-4cd9-4927-a35a-9e78569c7425",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed54bc8f-c3ef-45d0-a959-72ba73d0741b",
   "metadata": {},
   "source": [
    "## Set up the Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58eb929-8011-4544-8a55-3e7ae08b34c1",
   "metadata": {},
   "source": [
    "We'll use the Adam optimizer with a custom learning rate scheduler according to the formula in the original Transformer [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d2499-cd32-4cd9-8ca0-9dc6c41004ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    \n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192ba3f-6002-4ba7-8907-0790b3acdbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.98,\n",
    "    epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e33b9f-5f8e-4f39-ac9a-8eb36bd1affd",
   "metadata": {},
   "source": [
    "## Set up Loss and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea03887-fc17-4174-88b6-ed0e23389804",
   "metadata": {},
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss. Use the cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792fc98-20cc-48ad-8296-211028e0e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True,\n",
    "        reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "    \n",
    "    mask = label != 0\n",
    "    \n",
    "    match = match & mask\n",
    "    \n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa1371-86d6-43a8-9316-a3b3d3415ac3",
   "metadata": {},
   "source": [
    "## Train It!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b0dbe-9c85-40c6-ba41-6bbdd2824908",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d017c4-f845-4bad-b6b3-1cbd990cedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.fit(train_batches, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad39e5-93e7-4b4e-846b-39a67132f41e",
   "metadata": {},
   "source": [
    "Or actually maybe don't. It turns out that these models are really, really computationally expensive to train and will absolutely crash your Colab environment. On my home computer, it was taking about 80 hours per epoch to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

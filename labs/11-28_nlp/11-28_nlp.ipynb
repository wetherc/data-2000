{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11404037-d575-4661-9e8c-33a45996c66a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2484a84-ccb1-4f1a-9a9b-607564302b61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edc289b-ebc0-4f37-a2c2-27a8752f6e6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "> Natural language processing (NLP) is a subfield [...] concerned with the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data\n",
    ">\n",
    "> Patel, A. & Arasanipali, A. (2021). _Applied Natural Language Processing in the Enterprise._ O'Reilly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e44593-195f-4c24-aea3-5bed0ade1f33",
   "metadata": {},
   "source": [
    "Major applications include:\n",
    "  - Machine translation\n",
    "  - Speech recognition\n",
    "  - Question answering\n",
    "  - Text summarization\n",
    "  - Chatbots\n",
    "  - Sentiment analysis\n",
    "\n",
    "Multiple approaches:\n",
    "  - Rules based\n",
    "  - Classical statistics\n",
    "  - Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc09ae8-f944-4f71-94ce-156c2b26b9df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Major Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9f697-9fc3-4bd7-b22f-2b095e0f8ee3",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef4748-4997-4eb2-bd64-e8f6bd52abf7",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into its atoms: words, punctuation, symbols, numbers, etc. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9deb37-d1c8-493b-9de0-47efb7babc5d",
   "metadata": {},
   "source": [
    "![](assets/sentence.png)\n",
    "\n",
    "can be tokenized as\n",
    "\n",
    "![](assets/tokens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08613d64-8d49-43ed-a4cc-726ef32cc47b",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ee58c-f911-4393-bebb-bbd4cf854504",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the process of assigning, as the name suggests, parts of speech to each word token: noun, verb, adverb, adjective, etc.\n",
    "\n",
    "![](assets/pos.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075959f-19d5-4950-b876-056a1f9d3bb6",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995e56b-d378-41df-9a54-ffcc8f0270ca",
   "metadata": {},
   "source": [
    "Dependency parsing involves labeling the relationships between individual tokens within a span: for example, in our above example, we might specify that \"the apartment\" is the object of the verb \"robbed\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb27653-e7da-44cb-b286-2a0533a76716",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a8be2-084a-454c-b802-36f81f77c5ba",
   "metadata": {},
   "source": [
    "Chunking identifies multiple, related tokens into a single token. For example, \"New York City\" could be treated as a single token despite being three separate words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eace83-a7d8-4225-b0bf-0ebb5802b87f",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9209258-ae01-4c62-84c1-9c9c25be4a18",
   "metadata": {},
   "source": [
    "Named entity recognition is the process of assigning labels to known entities: names, locations, dates, currencies, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d4ecd-3d85-448f-b19e-d6483cf7d48f",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491da78e-3b90-4396-815e-40a07d8ab9cc",
   "metadata": {},
   "source": [
    "Language is weird! It's fun. Except for NLP. Because we need to understand that \"am\", \"will be\", \"was\", and \"have been\" are all actually just different conjucations of the same verb.\n",
    "\n",
    "Lemmatization refers to this process: instead of having to work with all word variants, we can just work directly with the word's base to simplify things a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf03f11-05d6-41fb-825f-9ee35b02ada9",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9932cf-2d40-472e-bf86-c878ebb6d9d1",
   "metadata": {},
   "source": [
    "Stemming is a similar proces to lemmatization, but a little bit simpler. Here, we're just reducing words to their stems (so creative in what we name these things). For example, \"big\", \"bigger\", and \"biggest\" would all be stemmed to \"big-\".\n",
    "\n",
    "We use stemming as a bit of a shortcut since it's substantially cheaper than lemmatization: we don't need to understand the part of speech, or any other metadata about the word or chunk. We can just take a hammer to things and break off all the bits at the end of the word that we don't like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5f3f2-a5c6-4361-9c20-4ec8a82bfcc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NLP with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fb0f8-9a31-4b34-91d7-09922279105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and create the English nlp object\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like trees, kangaroos and narwhals.\")\n",
    "\n",
    "print([elem for elem in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94115d8-c990-4efc-877e-1bb23f20ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[2:5].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1f730-8ee4-4ae9-b9ee-bc3b943b1a3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775446ef-8b96-42d1-aebd-942225e9e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia\"\n",
    "    \"were in extreme poverty. Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849227f1-00ee-4b29-bb35-9cef710cf350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"is_alpha:\", [token for token in doc if token.is_alpha])\n",
    "print(\"is_punct:\", [token for token in doc if token.is_punct])\n",
    "print(\"like_num:\", [token for token in doc if token.like_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2f321-b21e-4f4d-8494-b98a783e017c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Trained Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476b40c-234e-4703-bd6d-800fcc904521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Models that enable spaCy to predict linguistic attributes in context\n",
    "  - Part-of-speech tags\n",
    "  - Syntactic dependencies\n",
    "  - Named entities\n",
    "- Trained on labeled example texts\n",
    "- Can be updated with more examples to fine-tune predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2c14a-fd65-413d-b76a-52354263e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688da6c8-1f5e-4414-8cb9-c653e097ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f87900-d05b-44e2-8fc5-83499da27e60",
   "metadata": {},
   "source": [
    "We can also predict syntactic dependencies with this package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572c5c9-08e6-4c46-98be-d7b3cd9d7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da3dba-ff0b-464b-8c17-a439876994cf",
   "metadata": {},
   "source": [
    "![](assets/dependency-labels.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9abaa3-f8c3-46f2-8dcc-6e42671b1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859fe6f-3d74-4705-b5ea-6112b18834e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A GPE entity includes {spacy.explain(\"GPE\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6876e04e-68e8-492e-ac38-2fe448d67cb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rule-Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1528a7-ac2c-4a46-9704-770ba1e8445c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Rules vs regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5db31-4725-4685-92f2-9d005fbd3b91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Rules let us:\n",
    "  - Match on Doc objects, not just strings\n",
    "  - Match on tokens and token attributes\n",
    "  - Use a model's predictions\n",
    "  - Example: \"duck\" (verb) vs. \"duck\" (noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc150691-a277-475a-95fa-470dec0bc0b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Match patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27696c27-1946-4e19-9245-4e90163521a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "  - Lists of dictionaries, one per token\n",
    "  - Match exact token texts. E.g.,\n",
    "\n",
    "```\n",
    "[{'TEXT': 'iPhone', {'TEXT': 'X'}]\n",
    "```\n",
    "\n",
    "  - Match lexical attributes. E.g.,\n",
    "\n",
    "```\n",
    "[{'LOWER': 'iphone', {'LOWER': 'x'}]\n",
    "```\n",
    "\n",
    "  - Match any token attributes. E.g.,\n",
    "\n",
    "```\n",
    "[{'LEMMA': 'buy', {'POS': 'NOUN'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da30641-3690-4148-88a5-7c5488e4101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a pipeline and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e880970-51cf-471a-8f69-6867adf85506",
   "metadata": {},
   "source": [
    "In the above output, the tuple returned contains:\n",
    "  - `match_id`: hash value of the pattern name\n",
    "  - `start`: start index of matched span\n",
    "  - `end`: end index of matched span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37fce2d-f93f-41ec-9826-57943dc1d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matches(matches):\n",
    "    match_text = []\n",
    "    for match_id, start, end in matches:\n",
    "        # Get the matched span\n",
    "        matched_span = doc[start:end]\n",
    "        match_text.append(matched_span.text)\n",
    "    return match_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169d618-cad1-43d6-8f3e-89e923e8bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_matches(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c827b-1daa-4664-b943-0afbf06e0164",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041ab62-1ccf-4dc2-9a72-abaaa582e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"FIFA_PATTERN\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "print_matches(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d64fb1-409e-4f08-a543-ef9178479af3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Matching other token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86415e9f-1682-4d6c-b1a7-269b03a2f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"LOVE_PATTERN\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "print_matches(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ac99a-6328-466d-911c-960f330ff2a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Operators and quantifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27714a60-6bed-4c23-b9a0-20baf6692b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "# print(spacy.explain(\"DET\"))\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"BUY_PATTERN\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "print_matches(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c1c28-aa65-4beb-8ac9-1964de0df4bf",
   "metadata": {},
   "source": [
    "![](assets/operators.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13481e37-37e8-4e28-8f9b-97f8b45c1100",
   "metadata": {},
   "source": [
    "# Large-Scale Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ce4a7-7b21-40bb-9c89-e9f768c67306",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55855854-4d3c-4920-baa2-fd9528098098",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Shared vocab and string store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e98086-4f30-4d65-a249-2eb9eac2d535",
   "metadata": {},
   "source": [
    "  - `Vocab`: stores data shared across multiple documents\n",
    "  - To save memory, spaCy encodes all strings to **hash values**\n",
    "  - Strings are only stored once in the StringStore via `nlp.vocab.strings`\n",
    "  - String store: bidirectional **lookup table**\n",
    "\n",
    "```python\n",
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "```\n",
    "\n",
    "  - Hashes can't be reversed â€“ that's why we need to provide the shared vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73488dc0-870a-4611-8cab-d14c365fddc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lexemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c1ee6-978f-4f0a-bac5-b80e2899288a",
   "metadata": {},
   "source": [
    "- A `lexeme` object is an entry in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af0629-7fb9-4e41-8918-8e3558d5da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5430b-8b79-481f-abc7-442c3bbba8f1",
   "metadata": {},
   "source": [
    "  - Contains the context-independent information about a word:\n",
    "      - Word text\n",
    "      - Lexical attributes\n",
    "  - Does not contain context-dependent POS tags, dependencies, or entity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d0761-27e4-4b72-bfc4-e36e1444f2d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564517da-7751-489a-9f93-9b58fba12d29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f5282-fafc-409e-8c49-6edfb09eac8d",
   "metadata": {},
   "source": [
    "Prebuilt pipelines will have taken large corpora of text and created **embeddings**: by looking at which words do or don't co-occur, and how frequently, we can create proximity maps of how near or far other tokens in the corpus are to a given word.\n",
    "\n",
    "![](assets/word-cooccurrence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd27d4d-ad89-47e6-a49f-451bd569c384",
   "metadata": {},
   "source": [
    "This process gives us word embeddings that we can use to determine how similar or dissimilar from one another two arbitrary words are:\n",
    "\n",
    "![](assets/word-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c60433-2fb8-4f97-9338-ab91807b1f21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Semantic similarity in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4d6ac-2375-40c9-b945-85d5ca2b5815",
   "metadata": {},
   "source": [
    "  - spaCy can compare two objects and predict similarity\n",
    "  - `Doc.similarity()`, `Span.similarity()`, and `Token.similarity()`\n",
    "  - Take another object and return a similarity score (0 to 1)\n",
    "  - Important: needs a pipeline that has word vectors included, for example:\n",
    "    - âœ… en_core_web_md (medium)\n",
    "    - âœ… en_core_web_lg (large)\n",
    "    - ðŸš« NOT en_core_web_sm (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a407bd0-8311-4373-8313-a6fe33c848e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a425f98-b48a-417e-840d-d4d8f0baaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc3f24-5029-45d2-b9d6-b84a58214a73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Combining Predictions and Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690a3d8-0177-41ab-a09e-3aa04e0f5c4f",
   "metadata": {},
   "source": [
    "![](assets/statistical-vs-rule.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc3d2c-9032-4948-915a-303e04037c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42892bff-a864-40fd-9346-8a08cc58fe36",
   "metadata": {},
   "source": [
    "  - `PhraseMatcher` like regular expressions or keyword search â€“ but with access to the tokens!\n",
    "  - Takes `Doc` object as patterns\n",
    "  - More efficient and faster than the Matcher\n",
    "  - Great for matching large word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab7a36-129d-4b06-b363-2f9cfbcb11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", [pattern])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4d725-6932-4684-84bb-4527f411a6e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training and Updating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96138d-3faa-466d-9fe2-559cf076789e",
   "metadata": {},
   "source": [
    "  - The entity recognizer tags words and phrases in context\n",
    "  - Each token can only be part of one entity\n",
    "  - Examples need to come with context\n",
    "\n",
    "```python\n",
    "doc = nlp(\"iPhone X is coming\")\n",
    "doc.ents = [Span(doc, 0, 2, label=\"GADGET\")]\n",
    "```\n",
    "  \n",
    "  - Texts with no entities are also important\n",
    "\n",
    "```python\n",
    "doc = nlp(\"I need a new phone! Any tips?\")\n",
    "doc.ents = []\n",
    "```\n",
    "\n",
    "**Goal: teach the model to generalize**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caabd03-e387-4f24-be2e-a0db105db937",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7095b8e-a35a-49ce-a653-9c6d41c13754",
   "metadata": {},
   "source": [
    "  - Examples of what we want the model to predict in context\n",
    "  - Update an existing model: a few hundred to a few thousand examples\n",
    "  - Train a new category: a few thousand to a million examples\n",
    "    - spaCy's English models: 2 million words\n",
    "  - Usually created manually by human annotators\n",
    "  - Can be semi-automated â€“ for example, using spaCy's `Matcher`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca2882b-40e6-427b-aed7-9be55f21a9aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generate training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f7e86-7056-410c-99be-9bbfd79c7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a Doc with entity spans\n",
    "doc1 = nlp(\"iPhone X is coming\")\n",
    "doc1.ents = [Span(doc1, 0, 2, label=\"GADGET\")]\n",
    "# Create another doc without entity spans\n",
    "doc2 = nlp(\"I need a new phone! Any tips?\")\n",
    "\n",
    "docs = [doc1, doc2]  # and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c69555-cfa0-4a00-9cd7-8e646412f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c4e45-e20e-4d5c-872e-104ec118ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Create and save a collection of training docs\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"./train.spacy\")\n",
    "\n",
    "# Create and save a collection of evaluation docs\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"./dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac49fc5-87fe-4243-8e4c-37f850f3287a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176a4bc-5a5c-4e05-83ef-9541087cc1cd",
   "metadata": {},
   "source": [
    "  - spaCy can auto-generate a default config file for you\n",
    "  - interactive quickstart widget in the docs\n",
    "  - `init config` command on the CLI\n",
    "\n",
    "```bash\n",
    "$ python -m spacy init config \\\n",
    "    ./config.cfg \\\n",
    "    --lang en \\\n",
    "    --pipeline ner\n",
    "```\n",
    "\n",
    "  - `init config`: the command to run\n",
    "  - `config.cfg`: output path for the generated config\n",
    "  - `--lang`: language class of the pipeline, e.g. en for English\n",
    "  - `--pipeline`: comma-separated names of components to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1636524-bf13-4fc6-b031-b02ca2cf9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.9 -m spacy init config \\\n",
    "    ./config.cfg \\\n",
    "    --lang en \\\n",
    "    --pipeline ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e4bb9-0d70-4712-9f5e-8fac47248f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.9 -m spacy train \\\n",
    "    ./config.cfg \\\n",
    "    --output ./output \\\n",
    "    --paths.train train.spacy \\\n",
    "    --paths.dev dev.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd889b7c-83d8-49a5-b721-20d074d7b37f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading a trained pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db615012-3c6f-46ed-8a1c-ad3f577ca738",
   "metadata": {},
   "source": [
    "  - output after training is a regular loadable spaCy pipeline\n",
    "    - `model-last`: last trained pipeline\n",
    "    - `model-best`: best trained pipeline\n",
    "  - load it with `spacy.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3575e450-eb1f-4397-952d-8ab926d43b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"./output/model-best\")\n",
    "doc = nlp(\"iPhone 11 vs iPhone 8: What's the difference?\")\n",
    "print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

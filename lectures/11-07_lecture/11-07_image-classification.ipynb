{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea1fff2-fda0-4363-90bd-92b658b4a1ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84114f15-09b4-40aa-bc3b-fb0ae100476c",
   "metadata": {},
   "source": [
    "![](assets/cat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb4ea2-7b96-4c7c-873d-f943cbf8828c",
   "metadata": {},
   "source": [
    "![](assets/cat_angles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de0445-a100-4053-98f5-45f378a3ef83",
   "metadata": {},
   "source": [
    "## Enter Convolutional Neural Networks\n",
    "\n",
    "- CNN takes just the image's raw pixel data as input and \"learns\" how to extract these features\n",
    "- To start, the CNN receives an input feature map: a three-dimensional matrix\n",
    "    - The size of the first two dimensions corresponds to the length and width of the images in pixels.\n",
    "    - The size of the third dimension is 3 (corresponding to the 3 channels of a color image: red, green, and blue).\n",
    "\n",
    "\n",
    "### Convolution\n",
    "\n",
    "![](assets/convolution_overview.gif)\n",
    "\n",
    "Convolution layers take several hyperparameters:\n",
    "\n",
    "  - **Input Size**: How many pixels in height and width is the input image? \n",
    "  - **Padding**: How many blank pixels do we want to place around the border of our image?\n",
    "  - **Kernel Size**: What should the size of our sliding window be?\n",
    "  - **Stride**: How many pixels do we want to move for each new window?\n",
    "\n",
    "\n",
    "For each filter-tile pair, the CNN performs element-wise multiplication of the filter matrix and the tile matrix, and then sums all the elements of the resulting matrix to get a single value\n",
    "\n",
    "<img src=\"assets/filter_matrix.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cba612f-f7a4-4dc0-b540-faff691a2880",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"350\"\n",
       "            src=\"https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/practica/image-classification/conv_widget\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d984aa340>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "\n",
    "IFrame(\n",
    "    'https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/practica/image-classification/conv_widget',\n",
    "    width=900,\n",
    "    height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d71f17-0afc-4812-be2a-b2f0286b3564",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "After each convolution layer, we apply a ReLU activation function. Short version here is that in practice, it just empirically works best for this type of task.\n",
    "\n",
    "\n",
    "### Pooling\n",
    "\n",
    "Pooling is a way of downsampling our image (or rather, our convolved features) to generally decrease processing time. There are a few different strategies here, but **Max Pooling** is a commong one:\n",
    "\n",
    "![](assets/maxpool_animation.gif)\n",
    "\n",
    "As the name suggests, it just takes the maximum value of all elements in each window. Max pooling takes two parameters:\n",
    "\n",
    "  - **Size**: How many pixels should be sampled in each step?\n",
    "  - **Stride**: How many pixels should we move in the x- or y- direction with each new step? Note that we can set stride equal to the size in order to have non-overlapping samples\n",
    "\n",
    "### Flattening\n",
    "\n",
    "This layer converts a three-dimensional layer in the network into a one-dimensional vector to fit the input of a fully-connected layer for classification. For example, a 5x5x2 tensor would be converted into a vector of size 50.\n",
    "\n",
    "Typically the final layer in this part of the model uses a **softmax** activation function which outputs a probability value from 0 to 1 for each of the classification labels the model is trying to predict.\n",
    "\n",
    "![](assets/full_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b43f7-f176-424f-b318-8fec5a6b6f1a",
   "metadata": {},
   "source": [
    "### A final note: Data Augmentation\n",
    "\n",
    "![](assets/data_augmentation.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

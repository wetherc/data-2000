{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136c7b67-3ed0-47f5-ad86-dc5f03597b74",
   "metadata": {},
   "source": [
    "# DATA-2000 Midterm Exam\n",
    "\n",
    "## Recipe Rating Prediction\n",
    "\n",
    "For this exercise, we are going to use a dataset of recipes and their ratings, taken from [the website Epicurious](https://www.epicurious.com/recipes-menus).\n",
    "\n",
    "Our dataset contains basic information about the dish (its name, description, ingredients, and directions), as well as nutritional content (calories, protein, sodium, and fat contents). Based on this information, we want to try and predict how well or poorly the dish will be rated by users.\n",
    "\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "This midterm will be worth 15% of your total grade for this course. It will be graded out of 50 points, divided into 4 sections:\n",
    "\n",
    "  - Data Prep: 10 points\n",
    "    - 5 points will be awarded for the actual data cleaning (evaluating your Python code)\n",
    "    - 5 points will be awarded for the text commentary narrating your choices and explaining your rationale for the data quality checks that you chose to use\n",
    "  - Feature Engineering: 12 points\n",
    "    - 2 points will be awarded by default, but may be subtracted from if there are substantial errors in your data prep that reduce the quality of your engineered features\n",
    "    - 5 points will be awarded for the actual feature engineering (evaluating your Python code)\n",
    "    - 5 points will be awarded for the text commentary narrating your choices and explaining your rationale\n",
    "  - Model Building: 14 points\n",
    "    - 4 points will be awarded by default, but may be subtracted from if there are substantial errors in your feature engineering that reduce the quality of your model\n",
    "    - 5 points will be awarded for the actual model building (evaluating your Python code)\n",
    "    - 5 points will be awarded for the text commentary narrating your choices and explaining your rationale\n",
    "  - Model Validation/Evaluation: 14 points\n",
    "    - 4 points will be awarded by default, but may be subtracted from if there are substantial errors in your model building that negatively impact the validity of your model\n",
    "    - 5 points will be awarded for the actual model validation and evaluation (evaluating your Python code)\n",
    "    - 5 points will be awarded for the text commentary narrating your choices and explaining your rationale\n",
    "\n",
    "> **NOTE:** You will NOT be evaluated on whether you model actually makes accurate predictions or not\n",
    "\n",
    "\n",
    "## Using Additional Resources\n",
    "\n",
    "This is an open-resource exam. You may use any available resources as references. I will be available for any questions that you have during the exam.\n",
    "\n",
    "Remember that all work must still be your own, and that this exam is governed by the [Policy on Academic Honesty outlined in our course syllabus](https://docs.google.com/document/d/1Aoh7LvTKTEZO74eOsNhLzorkLtljkuchpg3ScNM_VEs/edit#heading=h.r0b18a8gh450).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51f16d-1e89-4fcc-9f3c-21c54959a86e",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "\n",
    "First, let's download our dataset and take a look at what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f55cb-4f8b-4cd5-9fa6-8e8eb019440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('https://cdn.c18l.org/full_format_recipes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f31c7-57c4-4282-9fec-d0318cc95891",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f92587-4c7b-4855-a309-1e9d61216cff",
   "metadata": {},
   "source": [
    "## Data Prep & Cleaning\n",
    "\n",
    "Perform any data quality checks and data cleaning that you believe is appropriate. Convert any categorical columns to numeric ones, if needed. Provide a narrative explanation of your choices to accompany any code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58af787-ce5a-41d3-8d6a-b13778af5d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e3c08c0-9acf-4118-93cb-eae3293d38ff",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Develop any new feature(s) that you feel may be relevant to a model. Provide a narrative explanation of your choices to accompany any code.\n",
    "\n",
    "To help, I've included a `column_builder()` utility function that will create a new boolean column based on whether a string of text appears in any of (1) the recipe title; (2) the recipe description; or (3) the recipe tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355f203-b6be-49d1-b79a-c951e1e33d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_builder(category: str, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataset[f'is_{category}'] = ((\n",
    "        dataset['categories'].str.contains(f'{category}', na=False, case=False)\n",
    "    ) | (\n",
    "        dataset['title'].str.contains(f'{category}', na=False, case=False)\n",
    "    ) | (\n",
    "        dataset['desc'].str.contains(f'{category}', na=False, case=False)\n",
    "    )).astype(int)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "categories = [\n",
    "    'easy',\n",
    "    'breakfast'\n",
    "    # Add any additional keywords here\n",
    "]\n",
    "\n",
    "for category in categories:\n",
    "    data = column_builder(category, data)\n",
    "\n",
    "data['is_easy'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f22d7a-e774-4875-b9f9-01f1bc4aa5d0",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Build a model (either a regression or a neural network) to predict a recipe's rating based on any relevant attributes that you defined in the prior steps.\n",
    "\n",
    "You may choose to predict rating as a continuous value (0.0 to 5.0), or as a categorical (low/medium/high or similar).\n",
    "\n",
    "Provide a narrative explanation of your choices to accompany any code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33ca42-ac43-4767-8c13-0c6749617f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b07d7b4a-9fbf-4040-bb6e-4143d991c2ec",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training your model, evaluate its performance. What metric(s) did you choose to optimize on? Would you say that your model performed well or poorly? How did you evaluate its performance to arrive at that conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33ce4e-1e66-4294-bbbf-bf2cc2325e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e699d33b-faac-4f37-8111-e646dc459d13",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Midterm Submission\n",
    "\n",
    "To submit this exam, in Canvas navigate to DATA-2000-51 > Assignments > Midterm Exam ([link](https://canvas.jcu.edu/courses/33514/assignments/407120)). You can either upload the `.ipynb` file directly to Canvas, or you can provide a link to the assignment on your GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
